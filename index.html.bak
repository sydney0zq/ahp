<!doctype html>
<html lang="en">
<head>
    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script async src="https://www.googletagmanager.com/gtag/js?id=UA-126534802-2"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());

      gtag('config', 'UA-126534802-2');
    </script>

    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- <link href='http://fonts.googleapis.com/css?family=Open+Sans:300italic,400,300' rel='stylesheet' type='text/css'> -->
    <link href='https://fonts.googleapis.com/css?family=Cantarell:100,300' rel='stylesheet' type='text/css'>
    <meta name="description" content="DAVIS, Densely Annotated VIdeo Segmentation">

    <title>DAVIS: Densely Annotated VIdeo Segmentation</title>

    <!-- Buttons -->
    <!-- <link rel="stylesheet" type="text/css" href="css/buttons/component.css" /> -->

	<link rel="stylesheet" href="https://yui-s.yahooapis.com/pure/0.6.0/pure-min.css">
    <!--[if lte IE 8]>
        <link rel="stylesheet" href="css/layouts/side-menu-old-ie.css">
    <![endif]-->
    <!--[if gt IE 8]><!-->
        <link rel="stylesheet" href="css/layouts/side-menu.css">
    <!--<![endif]-->

<!--     <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="96x96" href="/favicon-96x96.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png"> -->
</head>

<script type="application/ld+json">
{
  "@context":"http://schema.org/",
  "@type":"Dataset",
  "name":"DAVIS: Densely Annotated VIdeo Segmentation 2017",
  "datePublished":"2017-04-03",
  "description":"Video object segmentation dataset than contains 150 video sequences. Each sequence contains multiple objects and their high quality segmentation mask in each of the frames.",
  "url":"https://davischallenge.org/",
  "license":"http://creativecommons.org/licenses/by-nc/4.0",
  "sameAs":"https://davischallenge.org/",
  "keywords":[
     "segmentation",
     "video",
     "video object segmentation"
  ],
  "creator":[
  {
     "@type":"Person",
     "url": "http://jponttuset.cat/",
     "name":"Jordi Pont-Tuset",
     "email":"davisvideochallenge@gmail.com"
  },
  {
     "@type":"Person",
     "url": "https://fperazzi.github.io/",
     "name":"Federico Perazzi",
     "email":"davisvideochallenge@gmail.com"
  },
  {
     "@type":"Person",
     "url": "https://sergicaelles.com/",
     "name":"Sergi Caelles",
     "email":"davisvideochallenge@gmail.com"
  }],
  "distribution":{
        "@type":"DataDownload",
        "encodingFormat":"ZIP",
        "contentUrl":"https://davischallenge.org/davis2017/code.html"
  },
  "citation":[{
    "@type":"CreativeWork",
    "name":"The 2017 DAVIS Challenge on Video Object Segmentation",
    "identifier":"https://arxiv.org/abs/1704.00675"
  },
  {
    "@type":"CreativeWork",
    "name":"A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation",
    "identifier":"https://ieeexplore.ieee.org/abstract/document/7780454/"
  }

  ]
}
</script>


<style type="text/css">
    /*********************************
    The list of publication items
    *********************************/
    /* The list of items */
    .biblist { }

    /* The item */
    .biblist li { }

    /* You can define custom styles for plstyle field here. */


    /*************************************
     The box that contain BibTeX code
     *************************************/
    div.noshow { display: none; }
    div.bibtex {
        margin-right: 0%;
        margin-top: 1.2em;
        margin-bottom: 1.3em;
        border: 1px solid silver;
        padding: 0.3em 0.5em;
        background: #ffffee;
    }
    div.bibtex pre { font-size: 75%; overflow: auto;  width: 100%; }


    .content-subhead{color: black}
</style>

<script type="text/javascript">
 <!--
 // Toggle Display of BibTeX
 function toggleBibtex(articleid) {
     var bib = document.getElementById('bib_'+articleid);
     if (bib) {
         if(bib.className.indexOf('bibtex') != -1) {
             bib.className.indexOf('noshow') == -1?bib.className = 'bibtex noshow':bib.className = 'bibtex';
         }
     } else {
         return;
     }
 }
-->
</script>


<body>

<div id="layout">
    <!-- Menu toggle -->
    <a href="#menu" id="menuLink" class="menu-link">
        <!-- Hamburger icon -->
        <span></span>
    </a>

    <div id="menu">
        <div class="pure-menu">
            <ul class="pure-menu-list">
                <li style="border-bottom: 1px solid;"><a href="index.html" class="pure-menu-link pure-menu-selected">Home</a></li>
            </ul>

            <span class="pure-menu-heading" style="border-bottom: 1px solid;">DAVIS 2016</span>
            <ul class="pure-menu-list">
                <li><a href="davis2016/soa_compare.html" class="pure-menu-link">Benchmark</a></li>
                <li><a href="davis2016/browse.html" class="pure-menu-link">Explore</a></li>
                <li style="border-bottom: 1px solid;"><a href="davis2016/code.html" class="pure-menu-link">Download</a></li>
            </ul>

            <span class="pure-menu-heading" style="border-bottom: 1px solid;">DAVIS 2017</span>
            <ul class="pure-menu-list">
                <li><a href="davis2017/soa_compare.html" class="pure-menu-link">Benchmark</a></li>
                <li style="border-bottom: 1px solid;"><a href="davis2017/code.html" class="pure-menu-link">Download</a></li>
            </ul>

            <span class="pure-menu-heading" style="border-bottom: 1px solid;">Challenges</span>
            <ul class="pure-menu-list">
                <li><a href="challenge2017/index.html" class="pure-menu-link">Challenge 2017</a></li>
                <li><a href="challenge2018/index.html" class="pure-menu-link">Challenge 2018</a></li>
                <li><a href="challenge2019/index.html" class="pure-menu-link">Challenge 2019</a></li>
                <li><a href="challenge2020/index.html" class="pure-menu-link">Challenge 2020</a></li>
            </ul>
        </div>
    </div>

    <div id="main">
        <div class="header">
            <h1>DAVIS: Densely Annotated VIdeo Segmentation</h1>
            <h2>In-depth analysis of the state-of-the-art in video object segmentation</h2>
        </div>

        <div class="pure-g" style="margin-left:20px; margin-right:10px;margin-top:20px;">
            <div class="pure-u-1-6"><img src="images/teaser/2017/scooter-board.jpg" alt="" style="width:95%"></div>
            <div class="pure-u-1-6"><img src="images/teaser/2017/boxing-fisheye.jpg" alt="" style="width:95%"></div>
            <div class="pure-u-1-6"><img src="images/teaser/2017/dogs-scale.jpg" alt="" style="width:95%"></div>
            <div class="pure-u-1-6"><img src="images/teaser/2017/drone.jpg" alt="" style="width:95%"></div>
            <div class="pure-u-1-6"><img src="images/teaser/2017/planes-water.jpg" alt="" style="width:95%"></div>
            <div class="pure-u-1-6"><img src="images/teaser/2017/bmx-trees.jpg" alt="" style="width:95%"></div>
            </div>
        <div class="content">
             <div class="pure-u-1-1" style="">
            <div style="margin-right: 0%;padding: 0.3em 0.5em;border: 1px solid #ff0000;margin-top: 33px;width:100%;height:100%;background: #FFD6D6;">
                <p style="font-weight:bold">
                The DAVIS dataset has moved to maintenance mode. As a result, we do not plan to organize any further
                challenges and we will no longer update the benchmark leaderboards. However, we have integrated the
                existing results into <a href="https://paperswithcode.com/task/video-object-segmentation">paperswithcode</a>.
                You can enter the results for you latest paper in the corresponding task webpage:
                <ul >
                    <li style="margin-top: -15px"><a href="https://paperswithcode.com/task/semi-supervised-video-object-segmentation">Semi-supervised</a></li>
                    <li><a href="https://paperswithcode.com/task/interactive-video-object-segmentation">Interactive</a></li>
                    <li><a href="https://paperswithcode.com/task/unsupervised-video-object-segmentation">Unsupervised</a></li>
                </ul>
                 </p>
                <p style="font-weight:bold;margin-top: -15px">
                Also, we will continue running the evaluation servers in Codalab. We would like to thank the community
                for taking part in the challanges and we encourage everyone to keep using the datasets for video object
                segmentation or any other task!
                </p>
            </div>
             </div>
            <h3 class="content-subhead" style="margin-top:33px;margin-bottom:12px;color: red">Latest news!</h3>
            <div class="pure-u-1-1" style="margin-top: -20px">
                <ul>
                    <li><a>Publications and final leaderboards for the DAVIS 2020 Challenge <a href="challenge2030/publications.html">now published!</a></a></li>
                    <li><a>Detail for the <a href="challenge2020/index.html">2020 Challenge</a> have been published!</a></li>
                    <li><a>Codalab sites to evaluate test-dev for DAVIS 2017 <a href="https://competitions.codalab.org/competitions/20516" target="_blank">Semi-Supervised</a> and <a href="https://competitions.codalab.org/competitions/20515" target="_blank">Unsupervised</a> are up again!</a></li>
                    <li><a href="davis2017/code.html#unsupervised">Train, val, test-dev and test-challenge</a> sets for the <a href="challenge2019/unsupervised.html">DAVIS 2017 Unsupervised</a> dataset available!</li>
                    <li><a>Object categories for DAVIS 2017 <a href="davis2017/code.html#semantics">now available!</a></a></li>
                </ul>
            </div>
            <h2 class="content-subhead" style="margin-top:33px;margin-bottom:12px">Datasets</h2>
            <div class="pure-u-1-1">
                 <ul>
                    <li><a style="font-size:115%;font-weight:bold" href="davis2016/code.html">DAVIS 2016:</a>  In each video sequence a <a style="font-weight: bold">single</a> instance is annotated.</li>
                    <li><a style="font-size:115%;font-weight:bold" href="davis2017/code.html#semisupervised">DAVIS 2017 Semi-supervised:</a>  In each video sequence <a style="font-weight: bold">multiple</a> instances are annotated.</li>
                    <li><a style="font-size:115%;font-weight:bold" href="davis2017/code.html#unsupervised">DAVIS 2017 Unsupervised:</a>  In each video sequence <a style="font-weight: bold">multiple</a> instances are annotated.</li>
                 </ul>
                <p><a style="font-weight:bold">Semi-supervised</a> and <a style="font-weight:bold">Unsupervised</a> refer to the level of <a style="font-weight:bold">human interaction at
                    test time</a>, not during the training phase. In <a style="font-weight:bold">Semi-supervised</a>, better called human guided,
                    the segmentation mask for the objects of interest is provided in the first frame.
                    In <a style="font-weight:bold">Unsupervised</a>, better called human non-guided, no human input is provided.</p>
            </div>
            <h2 class="content-subhead" style="margin-top:33px;margin-bottom:12px">Publications</h2>
            <div class="pure-g">

                <div class="pure-u-1-6">
                    <img class="pure-img-responsive" style="margin-top:5px; width:100%;" src="images/teaser/montage-2019.jpg" alt="arXiv">
                </div>
                <div class="pure-u-5-6">
                <p style="margin-left:0.6cm;line-height: 1.4em;margin-top:0em;">
                    The 2019 DAVIS Challenge on VOS: Unsupervised Multi-Object Segmentation<br>
                    <a href="https://sergicaelles.com/" target="_blank"/>S. Caelles</a>,
                    <a href="http://jponttuset.github.io" target="_blank">J. Pont-Tuset</a>,
                    <a href="https://fperazzi.github.io/" target="_blank"/>F. Perazzi</a>,
                    <a href="http://montesg.xyz/" target="_blank"/>A. Montes</a>,
                    <a href="http://www.vision.ee.ethz.ch/~kmaninis/" target="_blank"/>K.-K. Maninis</a>, and
                    <a href="https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1" target="_blank">L. Van Gool</a>
                    <br>
                    <i>arXiv:1905.00737, 2019</i><br>
                    [<a href="https://arxiv.org/pdf/1905.00737" target="_blank">PDF</a>] [<a href="javascript:toggleBibtex('Caelles_arXiv_2019')">BibTex</a>]
                </p>
                </div>
            </div>

            <div id="bib_Caelles_arXiv_2019" class="bibtex noshow"><pre>@article{Caelles_arXiv_2019,
              author = {Sergi Caelles and Jordi Pont-Tuset and Federico Perazzi and Alberto Montes and Kevis-Kokitsi Maninis and Luc {Van Gool}},
              title = {The 2019 DAVIS Challenge on VOS: Unsupervised Multi-Object Segmentation},
              journal = {arXiv:1905.00737},
              year = {2019}
            }</pre></div>
<div class="pure-g">

                <div class="pure-u-1-6">
                    <img class="pure-img-responsive" style="margin-top:5px; width:100%;" src="images/teaser/montage-2018.jpg" alt="arXiv">
                </div>
                <div class="pure-u-5-6">
                <p style="margin-left:0.6cm;line-height: 1.4em;margin-top:0em;">
                    The 2018 DAVIS Challenge on Video Object Segmentation<br>
                    <a href="https://sergicaelles.com/" target="_blank"/>S. Caelles</a>,
                    <a href="http://montesg.xyz/" target="_blank"/>A. Montes</a>,
                    <a href="http://www.vision.ee.ethz.ch/~kmaninis/" target="_blank"/>K.-K. Maninis</a>,
                    <a href="http://www.vision.ee.ethz.ch/~yuhchen/" target="_blank"/>Y. Chen</a>,
                    <a href="https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1" target="_blank">L. Van Gool</a>,
                    <a href="https://fperazzi.github.io/" target="_blank"/>F. Perazzi</a>, and
                    <a href="http://jponttuset.github.io" target="_blank">J. Pont-Tuset</a>
                    <br>
                                       <i>arXiv:1803.00557, 2018</i><br>
                    [<a href="https://arxiv.org/abs/1803.00557" target="_blank">PDF</a>] [<a href="javascript:toggleBibtex('Caelles_arXiv_2018')">BibTex</a>]
                </p>
                </div>
            </div>

            <div id="bib_Caelles_arXiv_2018" class="bibtex noshow"><pre>@article{Caelles_arXiv_2018,
  author = {Sergi Caelles and Alberto Montes and Kevis-Kokitsi Maninis and Yuhua Chen and Luc {Van Gool} and Federico Perazzi and Jordi Pont-Tuset},
  title = {The 2018 DAVIS Challenge on Video Object Segmentation},
  journal = {arXiv:1803.00557},
  year = {2018}
}</pre></div>

            <div class="pure-g">

                <div class="pure-u-1-6">
                    <img class="pure-img-responsive" style="margin-top:5px; width:100%;" src="images/teaser/montage-2017.jpg" alt="arXiv">
                </div>
                <div class="pure-u-5-6">
                <p style="margin-left:0.6cm;line-height: 1.4em;margin-top:0em;">
                    The 2017 DAVIS Challenge on Video Object Segmentation<br>
                    <a href="http://jponttuset.github.io" target="_blank">J. Pont-Tuset</a>,
                    <a href="https://fperazzi.github.io/" target="_blank"/>F. Perazzi</a>,
                    <a href="https://sergicaelles.com/" target="_blank"/>S. Caelles</a>,
                    <a href="https://biomedicalcomputervision.uniandes.edu.co/" target="_blank"/>P. Arbel√°ez</a>,
                    <a href="http://www.ahornung.net" target="_blank">A. Sorkine-Hornung</a>, and
                    <a href="https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1" target="_blank">L. Van Gool</a>
                    <br>
                                       <i>arXiv:1704.00675, 2017</i><br>
                    [<a href="https://arxiv.org/abs/1704.00675" target="_blank">PDF</a>] [<a href="javascript:toggleBibtex('Pont-Tuset_arXiv_2017')">BibTex</a>]
                </p>
                </div>
            </div>

            <div id="bib_Pont-Tuset_arXiv_2017" class="bibtex noshow"><pre>@article{Pont-Tuset_arXiv_2017,
  author = {Jordi Pont-Tuset and Federico Perazzi and Sergi Caelles and Pablo Arbel\'aez and Alexander Sorkine-Hornung and Luc {Van Gool}},
  title = {The 2017 DAVIS Challenge on Video Object Segmentation},
  journal = {arXiv:1704.00675},
  year = {2017}
}</pre></div>




<div class="pure-g">
                <div class="pure-u-1-6">
                    <img class="pure-img-responsive" style="margin-top:5px; width:100%;" src="images/teaser/montage.jpg" alt="CVPR">
                </div>
                <div class="pure-u-5-6">
                <p style="margin-left:0.6cm;line-height: 1.4em;margin-top:0em;">
                    A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation<br>
                    <a href="https://fperazzi.github.io/" target="_blank"/>F. Perazzi</a>,
                    <a href="http://jponttuset.github.io" target="_blank">J. Pont-Tuset</a>,
                    <a href="https://www.inf.ethz.ch/personal/mcbrian/" target="_blank">B. McWilliams</a>,
                    <a href="https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1" target="_blank">L. Van Gool</a>,
                    <a href="http://www.disneyresearch.com/people/markus-gross" target="_blank">M. Gross</a>, and
                    <a href="http://www.ahornung.net" target="_blank">A. Sorkine-Hornung</a>
                    <br>
                    <i>Computer Vision and Pattern Recognition (CVPR) 2016</i><br>
                    [<a href="http://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Perazzi_A_Benchmark_Dataset_CVPR_2016_paper.pdf" target="_blank">PDF</a>] [<a href="files/davis_supplementary.pdf" target="_blank">Supplemental</a>] [<a href="javascript:toggleBibtex('Perazzi2016')">BibTex</a>]
                </p>
                </div>
            </div>
            <div id="bib_Perazzi2016" class="bibtex noshow"><pre>@inproceedings{Perazzi2016,<br/>  author = {F. Perazzi and J. Pont-Tuset and B. McWilliams and L. {Van Gool} and M. Gross and A. Sorkine-Hornung},<br/>  title = {A Benchmark Dataset and Evaluation Methodology for Video Object Segmentation},<br/>  booktitle = {Computer Vision and Pattern Recognition},<br/>  year = {2016}<br/>}</pre></div>

            <i>Please cite the relevant papers in your publications if DAVIS helps your research.</i>

<!--             <h2 class="content-subhead" style="margin-top:33px">Web Content</h2>
            <div class="pure-g">
                <div class="pure-u-1-2" style="text-align:center;font-family: 'Cantarell', sans-serif;">
                    <a href="https://github.com/fperazzi/davis" target="_blank"><button class="btn btn-6 btn-6a" style="width: 85%;margin-top:10px;">Code</button></a>
                </div>
                <div class="pure-u-1-2" style="text-align:center;font-family: 'Cantarell', sans-serif;">
                    <a href="https://graphics.ethz.ch/Downloads/Data/Davis/davis.zip" target="_blank"><button class="btn btn-6 btn-6a" style="width: 85%;margin-top:10px;">Datasets</button></a>
                </div>
            </div> -->


        <h2 class="content-subhead" style="margin-top:33px">Preview of the Annotations (Train and Val DAVIS 2017 Semi-supervised)</h2>
         <div class="pure-g">
                <div class="pure-u-1-1">
                    <video style="width:100%;height:auto" muted loop controls poster="images/teaser/2017/classic-car.jpg">
                        <source src="images/DAVIS-2017-TrainVal.mp4" type="video/mp4">
                    </video>
          <!-- <embed src="" wmode="transparent" type="application/x-shockwave-flash" width="100%" height="100%" allowfullscreen="true" title="DAVIS"> -->
                </div>
        </div>

            <h2 class="content-subhead" style="margin-bottom:0px">Benchmark State-of-the-Art</h2>
            <p style="margin-top:10px">
                Display the evaluation of the current State-of-the-Art segmentation tecniques in DAVIS; using the three presented measures in our work.
            </p>

            <h2 class="content-subhead" style="margin-bottom:0px;margin-top:35px;">Explore State-of-the-Art Results</h2>
            <p style="margin-top:10px;">
                Visualize the segmentation results for all state-of-the-Art techniques on all DAVIS 2016 images, right from your browser.
            </p>

            <h2 class="content-subhead" style="margin-bottom:0px;margin-top:35px;">Downloads</h2>
            <p style="margin-top:10px;margin-bottom:30px">
                Download the DAVIS images and annotations, pre-computed results from all techniques, and the code to reproduce the evaluation.
            </p>

            <h2 class="content-subhead" style="margin-bottom:0px;margin-top:35px;">Contributions from the community</h2>
            <p style="margin-top:10px;margin-bottom:30px">
            These are the papers and projects in which the community has augmented our datasets:
                <ul style="margin-top: -20px">
                <li>
                    <b>Referring expression annotations for DAVIS 2016 and DAVIS 2017</b><br>
                    It contains the referring expression given the first frame to two annotators in DAVIS 2016 and DAVIS 2017. Moreover, it contains the referring expression given the whole video to two annotators in DAVIS 2017.
                    <div class="pure-g">

                    <div class="pure-u-1-6">
                        <img class="pure-img-responsive" style="margin-top:5px;margin-left:40px; width:63%;" src="images/referring_expression_annotations.png" alt="arXiv">
                    </div>
                    <div class="pure-u-5-6">
                    <p style="margin-left:0.6cm;line-height: 1.4em;margin-top:0em;">
                        Video Object Segmentation with Language Referring Expressions<br>
                        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/anna-khoreva/" target="_blank"/>A. Khoreva</a>,
                        <a href="http://scholar.google.de/citations?user=GHpxNQIAAAAJ&hl=en" target="_blank"/>A. Rohrbach</a>, and
                        <a href="https://www.mpi-inf.mpg.de/departments/computer-vision-and-multimodal-computing/people/bernt-schiele/" target="_blank">B. Schiele</a>
                        <br>
                                           <i>ACCV 2018</i><br>
                        [<a href="https://www.mpi-inf.mpg.de/vos-language" target="_blank">Website</a>] [<a href="javascript:toggleBibtex('Khoreva_arXiv_2018')">BibTex</a>]
                    </p>
                    </div>
                </div>

                <div id="bib_Khoreva_arXiv_2018" class="bibtex noshow"><pre>@inproceedings{KhoRohrSch_ACCV2018,
                title = {Video Object Segmentation with Language Referring Expressions},
                author = {Khoreva, Anna and Rohrbach, Anna and Schiele, Bernt},
                year = {2018},
                booktitle = {ACCV}
                }</pre></div>
                </li>
                </ul>
                <ul style="margin-top: -20px">
                <li>
                    <b>Human eye fixation for DAVIS 2016.</b><br>
                    It contains human eye fixation data captured with professional equipment by twenty participants for all the frames in DAVIS 2016 Train and Val.
                    <div class="pure-g">

                    <div class="pure-u-1-6">
                        <img class="pure-img-responsive" style="margin-top:5px;margin-left:40px; width:70%;" src="images/human_eye_fixation.png" alt="arXiv">
                    </div>
                    <div class="pure-u-5-6">
                    <p style="margin-left:0.6cm;line-height: 1.4em;margin-top:0em;">
                        Learning Unsupervised Video Object Segmentation through Visual Attention<br>
                        <a href="https://sites.google.com/site/wenguanwangwwg/" target="_blank"/>W. Wang</a>,
                        <a href="" target="_blank"/>H. Song</a>,
                        <a href="" target="_blank"/>S. Zhao</a>,
                        <a href="http://iitlab.bit.edu.cn/mcislab/~shenjianbing/" target="_blank"/>J. Shen</a>,
                        <a href="" target="_blank"/>S. Zhao</a>,
                        <a href="https://sites.google.com/view/stevenhoi/home" target="_blank"/>S.C.H. Hoi</a>,
                        <a href="http://www.dabi.temple.edu/~hbling/" target="_blank"/>H. Ling</a>
                        <br>
                                           <i>CVPR 2019</i><br>
                        [<a href="https://github.com/wenguanwang/AGS" target="_blank">Website</a>] [<a href="javascript:toggleBibtex('Wenguan_2019')">BibTex</a>]
                    </p>
                    </div>
                </div>

                <div id="bib_Wenguan_2019" class="bibtex noshow"><pre>@inproceedings{Wang_2019_CVPR,
                title = {Learning Unsupervised Video Object Segmentation through Visual Attention},
                author = {Wang, Wenguan and Song, Hongmei and Zhao, Shuyang and Shen, Jianbing and Zhao, Sanyuan and Hoi, Steven Chu Hong and Ling, Haibin},
                year = {2019},
                booktitle = {CVPR}
                }</pre></div>
                </li>
                </ul>
                <ul style="margin-top: -20px">
                    <li>
                        <b>Shadow annotations for DAVIS 2016.</b><br>
                        It contains the annotation of object shadows in 29 videos of DAVIS 2016.
                        <div class="pure-g">

                            <div class="pure-u-1-6">
                                <img class="pure-img-responsive" style="margin-top:5px;margin-left:40px; width:70%;" src="images/shadow_annotation.png" alt="arXiv">
                            </div>
                            <div class="pure-u-5-6">
                                <p style="margin-left:0.6cm;line-height: 1.4em;margin-top:0em;">
                                    Temporally Coherent Completion of Dynamic Video<br>
                                    <a href="https://filebox.ece.vt.edu/~jbhuang/" target="_blank"/>J. Huang</a>,
                                    <a href="http://www.singbingkang.com/" target="_blank"/>S. B. Kang</a>,
                                    <a href="https://ece.illinois.edu/about/directory/faculty/n-ahuja" target="_blank"/>N. Ahuja</a>,
                                    <a href="https://johanneskopf.de/" target="_blank"/>J. Kopf</a>
                                    <br>
                                    <i>ACM 2016</i><br>
                                    [<a href="https://filebox.ece.vt.edu/~jbhuang/project/vidcomp/index.html" target="_blank">Website</a>] [<a href="javascript:toggleBibtex('Huang_2016')">BibTex</a>]
                                </p>
                            </div>
                        </div>

                <div id="bib_Huang_2016" class="bibtex noshow"><pre>@inproceedings{Huang_2016,
                title = {Temporally Coherent Completion of Dynamic Video},
                author = {Huang, Jia-Bin and Kang, Sing Bing and Ahuja, Narendra and Kopf, Johannes},
                year = {2016},
                booktitle = {ACM}
                }</pre></div>
                </li>
                </ul>
            </p>

            <i>Is your technique missing although it's published and the code is public? <a href='mailto:davisvideochallenge@gmail.com'>Let us know</a>  and we'll add it.</i>
        </div>
    </div>
</div>





<script src="js/ui.js"></script>
<!-- Start of StatCounter Code for Default Guide -->
<script type="text/javascript">
//<![CDATA[
var sc_project=11310740; 
var sc_invisible=1; 
var sc_security="0e2f84a6"; 
var scJsHost = (("https:" == document.location.protocol) ?
"https://secure." : "http://www.");
document.write("<sc"+"ript type='text/javascript' src='" +
scJsHost+
"statcounter.com/counter/counter_xhtml.js'></"+"script>");
//]]>
</script>
<noscript><div class="statcounter"><a title="website
statistics" href="http://statcounter.com/"
class="statcounter"><img class="statcounter"
src="//c.statcounter.com/11310740/0/0e2f84a6/1/"
alt="website statistics" /></a></div></noscript>
<!-- End of StatCounter Code for Default Guide -->

</body>
</html>
